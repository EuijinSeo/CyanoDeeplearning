{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black\"><span style = \"font-size:30px\"> CNN model training</span>\n",
    "&nbsp;&nbsp;&nbsp;\n",
    "   \n",
    "    \n",
    "In this process, we built a convolutional neural network model (CNN) to predict promoter strength. We used the training data set from the previous step (see the section of ‘Acquisition of promoter dataset')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tImport python modules required for training a CNN model and import the training dataset generated in the previous step (Acquisition of promoter dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import kFold\n",
    "import itertools\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "import theano\n",
    "from keras import optimizers\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "data = pd.read_excel('PCC6803 Promoter and reads 100bp.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tDefine the 'One-hot encoding' (OHE) function (see the section of ‘VAE model training’). The logarithmic scale (log2) of the reads from dRNA-seq is used as the promoter strength of each promoter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n",
    "\n",
    "def one_hot_encoding(df, seq_column, expression):\n",
    "    bases = ['A','C','G','T']\n",
    "    base_dict = dict(zip(bases,range(4)))\n",
    "    n = len(df)\n",
    "    total_width = df[seq_column].str.len().max()+20\n",
    "    X = np.zeros((n,1,4,total_width))\n",
    "    seqs = df[seq_column].values\n",
    "    for i in range(n):\n",
    "        seq = seqs[i]\n",
    "        for b in range(len(seq)):\n",
    "            X[i,0,base_dict[seq[b]], b+10+100-len(seq)] = 1    \n",
    "    X = X.astype(theano.config.floatX)\n",
    "    y = np.asarray(df[expression].values, dtype = theano.config.floatX)[:,np.newaxis]\n",
    "    return X, y, total_width\n",
    "\n",
    "Xtot, ytot, _ = one_hot_encoding(data,'Promoter','Reads')\n",
    "ytot= np.log2(ytot+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tDefine the 'hyperdict’. The 'hyperdict' is a variable set containing the number of kernels ('con1_num, con2_num'), the width of kernels ('con1_len', 'con2_len', 'den1_len'), the sign showing whether the computer builds the additional layer or not ('con2_prob', 'den_prob'), droplate ('droplate'), batch size ('batch_size') and epochs ('epochs')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.\n",
    "\n",
    "hyperdict = {'con1_num' : [4,8,16,32],'con1_len' : [6,12,18,24], 'con2_prob' : [0,1],\n",
    "             'con2_num' : [4,8,16,32],'con2_len' : [6,12,18,24],'den_prob' : [0,1],\n",
    "             'den1_len' : [4,8,16,32], 'droprate' : [0.1,0.2,0.3,0.5] , 'batch_size' :[32,64], 'epochs' : [50, 75,100, 150, 200]} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tDefine the 'create_model' function. The 'create_model' function returns the model constructed with the randomly chosen elements in ‘hyperdict’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.\n",
    "\n",
    "def create_model(con1n = None, con1l= None,con2p = None, con2n= None, con2l= None,denp = None,den1= None, dout = None):\n",
    "    model= models.Sequential()\n",
    "    model.add(layers.Conv2D(con1n,(4, con1l), activation = 'relu', data_format = 'channels_first', input_shape = (1,4,120)))\n",
    "    model.add(layers.Dropout(dout))\n",
    "    if con2p == 1:     \n",
    "        model.add(layers.Conv2D(con2n,(1,con2l),activation = 'relu' ,data_format = 'channels_first'))\n",
    "        model.add(layers.Dropout(dout))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(dout))\n",
    "    if denp == 1:\n",
    "        model.add(layers.Dense(den1, activation = 'relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tTrain the CNN model. This code helps you yield the best model with a minimum loss function value. In addition, to prevent possible overfitting during the training, we conduct k-fold cross-validation (k = 5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5.\n",
    "\n",
    "start = time.time() \n",
    "\n",
    "total_val = 10000\n",
    "trial = 100\n",
    "kfold = kFold(n_split = 5)\n",
    "oplist = []\n",
    "for _ in range(trial):\n",
    "    instant_val = 0\n",
    "    for X_train, y_train in kfold.split(X):\n",
    "        c1n = random.choice(hyperdict['con1_num'])\n",
    "        c1l = random.choice(hyperdict['con1_len'])\n",
    "        c2p = random.choice(hyperdict['con2_prob'])\n",
    "        c2n = random.choice(hyperdict['con2_num'])\n",
    "        c2l = random.choice(hyperdict['con2_len'])\n",
    "        denp =random.choice(hyperdict['den_prob'])\n",
    "        den1 = random.choice(hyperdict['den1_len'])\n",
    "        dout = random.choice(hyperdict['droprate'])\n",
    "        ep = random.choice(hyperdict['epochs'])\n",
    "        b_size = random.choice(hyperdict['batch_size'])\n",
    "        model = create_model(c1n,c1l,c2p,c2n,c2l,denp,den1,dout)\n",
    "        model.compile(optimizer = 'Adam', loss = 'mean_squared_error', metrics = ['mean_squared_error'])\n",
    "        history = model.fit(X_train, y_train, epochs = ep, verbose = 1, batch_size = b_size)    \n",
    "        instant_val += history.loss[-1]\n",
    "    if total_val >instant_val/5:\n",
    "        model.save('CNN_model.h5')\n",
    "print(\"time :\", time.time() - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
