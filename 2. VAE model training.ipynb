{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black\"><span style = \"font-size:30px\"> VAE model training</span>\n",
    "&nbsp;&nbsp;&nbsp;\n",
    "   \n",
    "    \n",
    "In this process, we built the variational autoencoder (VAE) model to generate new promoters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tImport python modules required for training a VAE model and load the training dataset generated in the previous section (Acquisition of promoter dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import theano\n",
    "from keras import optimizers\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import KFold\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "import time\n",
    "data = pd.read_excel('Traning dataset.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tDefine the 'One-hot encoding' (OHE) function. OHE is a method that converts categorical variables to numerical ones in an interpretable format. Through the OHE, we can vectorize the DNA sequences, i.e., A : [1,0,0,0], C : [0,1,0,0], G : [0,0,1,0], T : [0,0,0,1]. The 'one-hot encoding' function converts 100-base-pair promoter sequences to 1x1x4x100-sized tensors. To avoid information loss at both ends of a promoter during model training, we put the 1x1x4x10-sized zero tensor at each end to yield a 1x1x4x120-sized tensor for each promoter. Through the OHE for all 3712 promoters, you can obtain a 3712x1x4x120-sized tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n",
    "\n",
    "def one_hot_encoding(df, seq_column, expression):\n",
    "    bases = ['A','C','G','T']\n",
    "    base_dict = dict(zip(bases,range(4)))\n",
    "    n = len(df)\n",
    "    total_width = df[seq_column].str.len().max()+20\n",
    "    X = np.zeros((n,1,4,total_width))\n",
    "    seqs = df[seq_column].values\n",
    "    for i in range(n):\n",
    "        seq = seqs[i]\n",
    "        for b in range(len(seq)):\n",
    "            X[i,0,base_dict[seq[b]], b+10+100-len(seq)] = 1    \n",
    "    X = X.astype(theano.config.floatX)\n",
    "    return X, total_width\n",
    "\n",
    "X, total_width = one_hot_encoding(data,'Promoter','Reads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tDefine the class object of convolutional variational autoencoder (CVAE). The CVAE model contains ‘an encoder’ and ‘decoder’. The ‘encoder’ encodes the promoter tensor (1X1X4X120) into latent space. The ‘decoder’ converts a point of latent space into the generated data (1X1X4X120) showing genuine training dataset features. The size of the latent space can be specified by assigning a size number to the ‘latent_dim’ variable. The encoded inputs are converted into ‘mean’ and ‘logvar’ tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.\n",
    "\n",
    "class VAE(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=(1,4,120)),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=16, kernel_size=(4,35), strides=(1, 1), activation='relu',data_format = 'channels_first'),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=16, kernel_size=(1,21), strides=(1, 1), activation='relu',data_format = 'channels_first'),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=16, kernel_size=(1,15), strides=(1, 1), activation='relu',data_format = 'channels_first'),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(latent_dim + latent_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.decoder = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "                tf.keras.layers.Dense(units=1*4*120, activation=tf.nn.relu),\n",
    "                tf.keras.layers.Reshape(target_shape=(1, 4, 120)),\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=16, kernel_size=(1,15), strides=(1,1), padding='same',\n",
    "                    activation='relu',data_format = 'channels_first'),\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=16, kernel_size=(1,21), strides=(1,1), padding='same',\n",
    "                    activation='relu',data_format = 'channels_first'),\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=16, kernel_size=(1,35), strides=(1,1), padding='same',\n",
    "                    activation='relu',data_format = 'channels_first'),\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=1, kernel_size=(4,29), strides=1, padding='same', data_format = 'channels_first'),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(eps, apply_sigmoid=True)\n",
    "\n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.decoder(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tDefine the loss function and train steps. The 'log_normal_pdf' function is used to calculate the probability density function, and the 'compute_loss' function is used to calculate the Kullback-Leibler divergence (KLD). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    \n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "      axis=raxis)\n",
    "\n",
    "def compute_loss(model, x):\n",
    "    \n",
    "    mean, logvar = model.encode(x)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    x_logit = tf.dtypes.cast(model.decode(z),tf.float64)\n",
    "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "    logpz = tf.dtypes.cast(log_normal_pdf(z, 0., 0.),tf.float64)\n",
    "    logqz_x = tf.dtypes.cast(log_normal_pdf(z, mean, logvar),tf.float64)\n",
    "    \n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, x, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(model, x)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tSet up a model. You can set the number of epochs (how many times the training function runs) and the dimension of the latent space. You can also decide the number of synthetic promoters to be generated by assigning the desired number to the variable 'num_examples_to_generate'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.\n",
    "\n",
    "epochs = 1000\n",
    "latent_dim = 2\n",
    "num_examples_to_generate = 10000\n",
    "random_vector_for_generation = tf.random.normal(\n",
    "    shape=[num_examples_to_generate, latent_dim])\n",
    "model = VAE(latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tModel training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 6.\n",
    "\n",
    "X = np.expand_dims(X,axis = 0)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    start_time = time.time()\n",
    "    for tot_X in X:\n",
    "        train_step(model, tot_X, optimizer)\n",
    "    end_time = time.time()\n",
    "\n",
    "    loss = tf.keras.metrics.Mean()\n",
    "    for tot_X in X:\n",
    "        loss(compute_loss(model, tot_X))\n",
    "    elbo = -loss.result()\n",
    "    elbolist.append(elbo)\n",
    "    \n",
    "    \n",
    "def decoder2seq(x):\n",
    "    seq = ''\n",
    "    for i in range(np.shape(x)[-1]-20):\n",
    "        Ascalar = x[0][0][i+10].numpy()\n",
    "        Cscalar = x[0][1][i+10].numpy()\n",
    "        Gscalar = x[0][2][i+10].numpy()\n",
    "        Tscalar = x[0][3][i+10].numpy()\n",
    "        maxcalar = max(Ascalar, Cscalar, Gscalar, Tscalar)\n",
    "        if Ascalar == maxcalar:\n",
    "            seq = seq+'A'\n",
    "        elif Cscalar == maxcalar:\n",
    "            seq = seq+'C'\n",
    "        elif Gscalar == maxcalar:\n",
    "            seq = seq+'G'\n",
    "        else:\n",
    "            seq = seq+'T'\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Save the VAE model and synthetized promoter dataset. You can save the VAE model separately by the encoder ('cyano_encoder.h5') and decoder ('cyano_decoder.h5')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.\n",
    "\n",
    "model.encoder.save('cyano_encode.h5')\n",
    "model.decoder.save('cyano_decode.h5')\n",
    "\n",
    "aplist = []\n",
    "for i in range(10000):\n",
    "    aplist.append(decoder2seq(model.decode(random_vector_for_generation)[i]))\n",
    "    \n",
    "random_vector_for_generation = tf.random.normal(shape=[10000, 2])\n",
    "ddff = pd.DataFrame(aplist)\n",
    "ddff.columns = ['Generated promoters']\n",
    "ddff.to_excel('Generated promoter sequences 100bp.xlsx', index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
